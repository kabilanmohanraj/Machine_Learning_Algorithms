# -*- coding: utf-8 -*-
"""Personal_Loan_Data_Set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18-BjPJiOgdGJYJWyKxYmA3HDn0CfqiyR
"""

# Commented out IPython magic to ensure Python compatibility.
# To enable plotting graphs in Jupyter notebook
# %matplotlib inline

"""# Import all the required packages and read the dataframe into a variable called bank_df"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

bank_df = pd.read_csv('Personal_Loan_Modelling.csv')
bank_df.head()

"""# Explore the dataset  and do the neccessary preprocessing for the attributes and state your insights"""

bank_df.shape

bank_df.info()

"""### From the dataset:
##### The dataset is found to have both numerical and categorical columns
##### 6 categorical, 1 numerical(to be treated as categorical) - ZIP code
##### ID column is of no use
##### 6 numerical columns
### The target column is 'Personal Loan'. This was infered from the problem statement.
### The dataset is highly skewed towards people who have not opted personal loan. Only 480 out of 5000 have taken a loan
"""

bank_df.describe().T

"""### Inferences from the above data:
##### The mean and median were found to be more or less the same for the numerical columns
##### Income column is highly skewed towards a higher income value
##### Checking the statistical information after preprocessing will give us a better understanding

### Discripencies found:
1. Person with no credit card is has non-zero CCAvg(Average amount spent on credit card)
2. Experience in years is negative
"""

temp = bank_df[['Income', 'CCAvg', 'Mortgage']]
temp.plot(kind='box', figsize=(12,12))

"""#### The above box plots imply the high biasness in certain columns. These columns were plotted because they were found to have skewness and possible outliers from the "bank_df.describe()" command."""

for i in ['Income', 'CCAvg', 'Mortgage']:    
    plt.figure()
    sns.distplot(bank_df[i])

"""#### This plot is to check for the skewness and to check the biasness in the data."""

# Person with no credit card is has non-zero CCAvg
bank_df['CCAvg'] = bank_df['CCAvg']*bank_df['CreditCard']
bank_df.head()

# The discrepency was found to be removed

"""##### Dropping the negative values could be a solution for the discripency 2. But vouching to a customer who would not be interested is better than not vouching to a potential customer. Therefore, we can take the absolute value of the negative values. There is also the chance of the positive value being entered as a negative value(human data entry error)."""

# Experience in years is negative
bank_df['Experience'] = bank_df['Experience'].abs()
bank_df['Experience'].min()

# The discrepency was found to be removed

"""##### Checking whether the family column has any 0 values(no member in the family). No issues were found."""

bank_df['Family'][bank_df['Family']==0].count()

"""##### Checking whether to encode the zipcode column. All the 5000 people are from one of these 467 regions. This could play a vital role in deciding which area to target inorder to get more customers. Therefore, we can encode our ZIP code column.
##### Onehot encoding cannot be done because of a high quantity of unique values
"""

bank_df['ZIP Code'].unique().shape

from sklearn.preprocessing import LabelEncoder

bank_df.columns

label_encoder = LabelEncoder()
bank_df['ZIP Code'] = label_encoder.fit_transform(bank_df['ZIP Code'])
bank_df.head()

"""##### Now scaling has to be done because the column values are of different ranges. This will lead to wrong feature selection when variance is considered without scaling."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
for column in ['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Education', 'Mortgage']:
    bank_df[column] = scaler.fit_transform(bank_df[[column]])

bank_df.head()
# The values have been scaled to the range 0-1

"""#### Removal of outliers -- The general notion is that people with high income would not opt for personal loan. This idea can be used as the base for not removing the outliers. The higher the income, the lower the chance of them getting a loan.

Before outlier removal:
    Count of people who took perosnal loan = 480

After outlier removal:
    Count of people who took perosnal loan = 438
    
### This proves that removing outliers further increases the bias in the dataset. Therefore, outliers were not removed. It also proves that people with high income also take personal loans which disproves the above general notion.
"""

# bank_df['Personal Loan'].sum()

# for outlier in ['Income']:
#     q1 = bank_df[outlier].quantile(0.25)
#     q3 = bank_df[outlier].quantile(0.75)
#     iqr = q3-q1
#     a = q1-1.5*iqr
#     b = q3+1.5*iqr
#     bank_df = bank_df[bank_df[outlier]>a]
#     bank_df = bank_df[bank_df[outlier]<b]

bank_df.shape

"""# Separate the independent attributes and store them in X array. Store the target column into Y array"""

data_columns = list(bank_df.columns)
data_columns

"""### It was infered from the problem statement that 'Personal Loan' is the target column"""

bank_df.describe().T

X = bank_df[['Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg', 'Education', 'Mortgage', 'Securities Account', 'CD Account', 'Online', 'CreditCard']]
y = bank_df[['Personal Loan']]

"""# Apply Principal Component Analysis on the explored attributes and select the features that are explaining 95% variance of the data distribution and show the feature importance"""

from sklearn.decomposition import PCA

pca = PCA(0.95)

pca_data = pca.fit_transform(X)

pca_data.shape

pca.explained_variance_ratio_

"""#####  Here, just one column explains 99% of the vairance. This may lead to over-fitting of the model to the given data. But the following accuracy scores imply that the model was not over-fit upon the data

# Create the training and test data set in the ratio of 70:30 respectively and set the random_state as 42.
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(pca_data, y, test_size=0.3, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# Build a Logistic Regression Model to predict the personal loan affinity"""

from sklearn.linear_model import LogisticRegression

logistic_regression = LogisticRegression()
logistic_regression.fit(X_train, y_train)

print(logistic_regression.score(X_train, y_train), logistic_regression.score(X_test, y_test))

"""# Print the confusion matrix and state your insights about the performance of the model"""

from sklearn.metrics import confusion_matrix

y_pred = logistic_regression.predict(X_test)
a = confusion_matrix(y_test, y_pred, labels=[0, 1])

y_pred.sum()

"""#### From the confusion matrix we can see the Logistic regression model predicted that, no customer will take a personal loan. Even then the accuracy is higher than that of other models because of the high biasness of the data and not because of the performance of the model.

#### True Positive - 157
#### True Negative - 1343

# Build a k Nearest Neigbours Classifier Model to predict the personal loan affinity (Provide the k Value as 3)
"""

from sklearn.neighbors import KNeighborsClassifier

model_knn = KNeighborsClassifier(n_neighbors=3)
model_knn.fit(X_train, y_train)

print(model_knn.score(X_train, y_train), model_knn.score(X_test, y_test))

"""# Print the confusion matrix and state your insights about the performance of the model"""

y_pred = model_knn.predict(X_test)
b = confusion_matrix(y_test, y_pred, labels=[0, 1])

y_pred.sum()

"""#### From the confusion matrix we can see that the KNN model has performed better than the above Logistic regression model. This has predicted 1 potential customer who might take loan and misclassified 156 potential customers. This may prove as a problem for the bank.

# Apply Random Forest Classifier Method and keep the number of estimators as 10 and check if there is any improvement in the model performance. State your insights
"""

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier(n_estimators=10, random_state=42)
random_forest.fit(X_train, y_train)

print(random_forest.score(X_train, y_train), random_forest.score(X_test, y_test))

y_pred = random_forest.predict(X_test)
c = confusion_matrix(y_test, y_pred, labels=[0, 1])

y_pred.sum()

"""#### From the confusion matrix we can see that the random forest model has performed better than the above Logistic regression model and KNN model. This has predicted 9 potential customer who might take loan and misclassified 148 potential customers. This may prove as a problem for the bank.

# Compare the performance of the three models you have designed and state your conclusions so as to finalize which model can be taken for solving the problem statement
"""

final_array = [a[1,0], b[1,0], c[1,0]]
final_array

pd.DataFrame(final_array, index=['lr', 'knn', 'rf']).plot(kind='bar')

"""### For the above problem if we go by accuracy to choose a model, we will choose Logistic regression model. But the classification was done wrongly as infered from the confusion matrix. So, this implies that which model gives the least Type-2 error has to be chosen. In this scenario, it is the Random Forest model. One more reason to choose this model is because the underlying concept in this model is Decision Tree. The decision tree classifier is data dependent and not function dependent like Logistic regression.

#### Why least type-2 error? -- Potential customers are classified as csutomers who will not take loan resulting in lost revenue as compared to when a non-potential customer is called.
"""