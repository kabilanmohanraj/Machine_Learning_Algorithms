# -*- coding: utf-8 -*-
"""Gender_Recognition_by_Voice_Data_Set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HEQWcJFHN2dF_ds-7Uw34bTzs3yXzKu3
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import LabelEncoder

df_voice=pd.read_csv('voice-classification.csv')

df_voice.shape

df_voice.head()

df_voice.info()

df_voice.isnull().sum()

cols=df_voice.columns

for i in cols:
    print(i,df_voice[df_voice[i]==0].count()[0])

df_voice.describe().T

df_voice.var()

df_voice_orig=pd.read_csv('voice-classification.csv')
df_voice_orig.head()

df_voice_orig[df_voice_orig['label']=='male'].count()[0]

le=LabelEncoder()
le.fit(df_voice['label'])
df_voice['label']=le.transform(df_voice['label'])

sns.heatmap(df_voice.corr())

"""#### y-label x-sfm,sp.ent,iqr,sd --> based on the correlation values

#### sfm is having high variance than sp.ent so we drop sp.ent

#### based on variance and correlation we have choosen 3 features to predict the label('Male','Female')

#### 0-male 1-female
"""

df_voice[df_voice['label']==0].count()[0]

X=df_voice[['sfm','IQR','sd']]
y=df_voice['label']

X_test,X_train,y_test,y_train=train_test_split(X,y,test_size=0.2,random_state=42)

"""### KNN"""

model=KNeighborsClassifier(n_neighbors=3)
model.fit(X_train,y_train)
pred=model.predict(X_test)
model.score(X_train,y_train)

model.score(X_test,y_test)

mean_squared_error(y_test,pred)

"""### SVM"""

model_svc=SVC(kernel="rbf")
model_svc.fit(X_train,y_train)
pred=model_svc.predict(X_test)
model_svc.score(X_train,y_train)

model_svc.score(X_test,y_test)

mean_squared_error(y_test,pred)

"""### Decision Tree"""

model_tree=DecisionTreeClassifier()
model_tree.fit(X_train,y_train)
pred=model_tree.predict(X_test)
model_tree.score(X_train,y_train)

model_tree.score(X_test,y_test)

mean_squared_error(y_test,pred)

"""### Logistic Regression"""

#6,9,13
X=df_voice[['IQR','sp.ent','meanfun']]
y=df_voice['label']

X_test,X_train,y_test,y_train=train_test_split(X,y,test_size=0.2,random_state=42)

model_log=LogisticRegression()
model_log.fit(X_train,y_train)
pred=model_log.predict(X_test)
model_log.score(X_train,y_train)

model_log.score(X_test,y_test)

mean_squared_error(y_test,pred)

"""#### selecting feature for logistic regression"""

from sklearn.feature_selection import RFE

model_log=LogisticRegression()

selector=RFE(model_log,3)

x=df_voice.loc[:,:'modindx']
y=df_voice['label']

x.shape

selector=selector.fit(x,y)

selector.support_

"""#### 6,9,13 using feature selection

#### selecting feature for svm
"""

model_svc=SVC(kernel="rbf")

selector2=RFE(model_svc,3)

x=df_voice.loc[:,:'modindx']
y=df_voice['label']

selector2=selector2.fit(x,y)

"""#### KNN using RFE"""

model_knn=KNeighborsClassifier(3)

selector3=RFE(model_knn,3)

selector3=selector3.fit(x,y)

"""#### decision tree using selector (RFE)"""

model_dtree=DecisionTreeClassifier()

selector4=RFE(model_dtree,3)

selector4=selector4.fit(x,y)

selector4.support_

6,10,12
df_voice.columns

X=df_voice[['sfm','IQR','centroid']]
y=df_voice['label']

X_test,X_train,y_test,y_train=train_test_split(X,y,test_size=0.2,random_state=42)

model_tree=DecisionTreeClassifier()
model_tree.fit(X_train,y_train)
pred=model_tree.predict(X_test)
model_tree.score(X_train,y_train)

model_tree.score(X_test,y_test)

"""#### Logistic Regression score after feature selection using RFE - 0.8804262036306235

## ensemble methods

### Bagging
"""

from sklearn.ensemble import RandomForestClassifier

ensemble_model=RandomForestClassifier(n_estimators=20,min_samples_split=20,min_impurity_decrease=0.05)

ensemble_model.fit(X_train,y_train)

print(ensemble_model.score(X_train,y_train),ensemble_model.score(X_test,y_test))

import numpy as np
features=pd.DataFrame(np.array([X.columns,ensemble_model.feature_importances_]).T,columns=["feature","importance"])
features

"""1.Logistic Regression score after feature selection using RFE - 0.8804262036306235

2.Logistic Regression score after bagging - 0.8843725335438043

##### concln:ensemble helps in improvising the score 

If you increase the no. of estimators and sample split you might get more accuracy

### Boosting
"""

from sklearn.ensemble import AdaBoostClassifier

ensemble_boost_model=AdaBoostClassifier(n_estimators=10)

ensemble_boost_model.fit(X_train,y_train)
print(ensemble_boost_model.score(X_train,y_train),ensemble_boost_model.score(X_test,y_test))

"""3.Logistic regression score after boosting - 0.8977900552486188

##### score is still improved

### cross validation kfolds

### knn with kfold
"""

from sklearn.model_selection import KFold

k_fold=KFold(n_splits=5,shuffle=False,random_state=40)

k_fold.split(X,y)

for i,(train,value) in enumerate(k_fold.split(X,y)):
    print(i)
    print('-------------------------------')
    print(X.iloc[train].shape,y.iloc[train].shape)
    print('-------------------------------')
    print(X.iloc[value].shape,y.iloc[value].shape)
    print('-------------------------------')

model_score_train=[]
model_score_test=[]
for i,(train,value) in enumerate(k_fold.split(X,y)):
    model_knn=KNeighborsClassifier()
    model_knn.fit(X.iloc[train],y.iloc[train])
    pred=model_knn.predict(X.iloc[value])
    model_score_train.append(model_knn.score(X.iloc[train],y.iloc[train]))
    model_score_test.append(model_knn.score(X.iloc[value],y.iloc[value]))
    print(model_knn.score(X.iloc[train],y.iloc[train]),model_knn.score(X.iloc[value],y.iloc[value]))

np.mean(model_score_test)

"""### decision tree with kfold"""

model_score_train1=[]
model_score_test1=[]
for i,(train,value) in enumerate(k_fold.split(X,y)):
    model_dtree=DecisionTreeClassifier()
    model_dtree.fit(X.iloc[train],y.iloc[train])
    pred=model_dtree.predict(X.iloc[value])
    model_score_train1.append(model_dtree.score(X.iloc[train],y.iloc[train]))
    model_score_test1.append(model_dtree.score(X.iloc[value],y.iloc[value]))
    print(model_dtree.score(X.iloc[train],y.iloc[train]),model_dtree.score(X.iloc[value],y.iloc[value]))

np.mean(model_score_test1)

"""### svm with kfold"""

model_score_train2=[]
model_score_test2=[]
for i,(train,value) in enumerate(k_fold.split(X,y)):
    model_svc=SVC()
    model_svc.fit(X.iloc[train],y.iloc[train])
    pred=model_svc.predict(X.iloc[value])
    model_score_train2.append(model_svc.score(X.iloc[train],y.iloc[train]))
    model_score_test2.append(model_svc.score(X.iloc[value],y.iloc[value]))
    print(model_svc.score(X.iloc[train],y.iloc[train]),model_svc.score(X.iloc[value],y.iloc[value]))

np.mean(model_score_test2)

"""### logistic with kfold"""

model_score_train3=[]
model_score_test3=[]
for i,(train,value) in enumerate(k_fold.split(X,y)):
    model_log=LogisticRegression()
    model_log.fit(X.iloc[train],y.iloc[train])
    pred=model_log.predict(X.iloc[value])
    model_score_train3.append(model_log.score(X.iloc[train],y.iloc[train]))
    model_score_test3.append(model_log.score(X.iloc[value],y.iloc[value]))
    print(model_log.score(X.iloc[train],y.iloc[train]),model_log.score(X.iloc[value],y.iloc[value]))

np.mean(model_score_test3)

df_score=pd.DataFrame(data=[model_score_test3,model_score_test2,model_score_test1,model_score_test],index=["Logistic","SVM","Dtree","KNN"],columns=['s1','s2','s3','s4','s5'])
df_score

df_score=df_score.T

df_score.boxplot(figsize=(10,5))

"""1.Logistic Regression is properly fitted with the given dataset but test accuracy is less
2.KNN have high accuracy but not properly fitted

### PCA
"""

from sklearn.decomposition import PCA

pca=PCA(n_components=3)
X_new=pca.fit_transform(x)
X_new.shape

new_log=LogisticRegression()

new_log.fit(X_new,y)
new_log.score(X_new,y)

pca.score(X_test,y_test)

x.var()

#amount of variance by each component 
pca.explained_variance_

#percentage of variance explained by selected components
pca.explained_variance_ratio_

from sklearn import preprocessing
data_scaled=pd.DataFrame(preprocessing.scale(X),columns=X.columns)

print(pd.DataFrame(pca.components_,columns=data_scaled.columns))

"""#### pc-1 is sfm pc-2 is IQR pc-3 is centroid"""

pca.components_

"""#### LDA Linear Discriminant Analysis"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda=LDA(n_components=5)

X_new=lda.fit_transform(X_train,y_train)
X_new.shape
#lda.transform(X_test)

lda.coef_

